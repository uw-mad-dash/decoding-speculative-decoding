{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b81d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "import deepspeed\n",
    "import torch.distributed as dist\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "world_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n",
    "dist.init_process_group(backend='nccl', rank=local_rank, world_size=world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b196d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tensors_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Input: file_path: Path to the file containing the ground truth tensors from large LLM execution. \n",
    "    \n",
    "    This demo is for those who doesn't have the resource to execute large LLM but wish to deploy and test speculative decoding.\n",
    "    \n",
    "    File format looks like the following:\n",
    "    tensor([[  518, 799, 596, 18873,  1265,   322,  1510,   372,  1283,   304,   596,\n",
    "          7875, 29991, 2]])\n",
    "    tensor([[  518, 10568, 29962,  1522,  2924, 29889,   518, 10568, 29962,  1522,\n",
    "          1176,   681, 29889,   518, 10568, 29962,  1522,   752,   465,   291]])\n",
    "    \n",
    "    Output: This function returns a list of tensor to be used by the draft LLM.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    # Find all tensor strings using regular expression\n",
    "    tensor_strings = re.findall(r\"tensor\\(\\[\\[([\\s\\S]+?)\\]\\]\\)\", data)\n",
    "\n",
    "    tensor_list = []\n",
    "    iter = 0 \n",
    "    prev_length = 0\n",
    "    for tensor_str in tensor_strings:\n",
    "        # Clean up the tensor string and split into rows\n",
    "        tensor_rows = tensor_str.strip().split('\\n')\n",
    "        \n",
    "        # Create a list of tensors, one for each row\n",
    "        row_tensors = []\n",
    "        for row in tensor_rows:\n",
    "            # Here we strip to remove spaces and trailing commas\n",
    "            row = row.strip().rstrip(',').lstrip(',')\n",
    "            if row:  # Make sure row is not empty\n",
    "                # Convert row string to a list of integers\n",
    "                tensor_values = [int(value) for value in row.split(',')]\n",
    "                # Convert the list to a tensor and append to row_tensors\n",
    "                row_tensors.append(torch.tensor(tensor_values))\n",
    "                # print(row_tensors)\n",
    "\n",
    "        # Create a tensor from the list of lists and add to our tensor list\n",
    "        final_tensor = torch.cat(row_tensors, dim=0)\n",
    "        if final_tensor.size(0) != prev_length:\n",
    "            print(iter, final_tensor.size(0), prev_length)\n",
    "        prev_length = final_tensor.size(0)\n",
    "        tensor_list.append(torch.cat(row_tensors, dim=0))\n",
    "\n",
    "        iter += 1\n",
    "    return tensor_list\n",
    "\n",
    "input_file_path = 'llama-65b-hellaswag.txt'  # Replace with your input file path\n",
    "ground_truth_tensor_list = parse_tensors_from_file(input_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e8af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_loader(file_path):\n",
    "    \"\"\"\n",
    "    Input: file_path: Path to the json file containing all the queries. \n",
    "        \n",
    "    File format looks like the following:\n",
    "    {\"prompt\": \"A seated man cleans a shoe in a classroom setting with other individuals. the man\"}    \n",
    "    {\"prompt\": \"Two girls are sailing on a lake. they\"}\n",
    "    \n",
    "    Output: This function returns a list of prompts to be used by the draft LLM.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "\n",
    "test_json = json_loader(\"./hellaswag.json\") # Replace with your file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a08bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LLaMA tokenizer does not have a pad token. \n",
    "# Modify the tokenizer to add a pad token and change the model configs accordingly.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yahma/llama-7b-hf\", padding_side=\"left\", torch_dtype=torch.float16)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "# Feel free to change it to the draft model of your choice\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\"minghaoyan/Wide-Sheared-LLaMA-796M\", torch_dtype=torch.float16)\n",
    "\n",
    "draft_model.resize_token_embeddings(len(tokenizer))\n",
    "draft_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "draft_model.embed_tokens = nn.Embedding(draft_model.config.vocab_size, draft_model.config.hidden_size, padding_idx=draft_model.config.pad_token_id)\n",
    "\n",
    "\n",
    "# Launch the draft model with deepspeed on 1 node. Alternatively, you could use HF or load from a checkpoint.\n",
    "draft_model = deepspeed.init_inference(\n",
    "                draft_model,\n",
    "                replace_with_kernel_inject=False,\n",
    "                tp={\"tp_size\": 1,},\n",
    "                dtype=torch.float16,\n",
    "                #checkpoint=checkpoint_dict,\n",
    "               )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_prompts = []\n",
    "curr_count = 0\n",
    "\n",
    "# Set hyperparameters for speculative decoding\n",
    "batch_size = 1\n",
    "max_new_tokens = 7\n",
    "output_file = \"output-file.txt\"\n",
    "\n",
    "for batch in test_json:\n",
    "    \n",
    "    # Constructing the prompt for each question\n",
    "    current_prompts.append(batch['prompt'])\n",
    "\n",
    "    curr_count += 1\n",
    "    if curr_count == batch_size:\n",
    "        \n",
    "        draft_input_ids = tokenizer.batch_encode_plus(current_prompts, padding='longest')\n",
    "        current_prompts = []\n",
    "        curr_count = 0\n",
    "        \n",
    "        if batch_size == 1:\n",
    "            ground_truth_slice = ground_truth_tensor_list[processed_batches]\n",
    "            ground_truth_tensor = ground_truth_slice.unsqueeze(0).cuda(local_rank)\n",
    "        else:\n",
    "            ground_truth_slice = ground_truth_tensor_list[processed_batches*batch_size:(processed_batches+1)*batch_size]\n",
    "            ground_truth_tensor = torch.stack(ground_truth_slice, dim=0).cuda(local_rank)\n",
    "\n",
    "        max_length = ground_truth_tensor.size(1) - max_new_tokens - 2\n",
    "        current_length = 0\n",
    "        iter_count = 0\n",
    "\n",
    "\n",
    "        total_matched = torch.zeros(batch_size, dtype=torch.int32).cuda(local_rank)\n",
    "\n",
    "        while current_length < max_length:\n",
    "\n",
    "            # The first iteration uses in the input prompt\n",
    "            # The following iterations use input constructed from the last iteration based on matched tokens\n",
    "            if iter_count == 0:\n",
    "                iter_count += 1\n",
    "\n",
    "                output_len = len(draft_input_ids[\"input_ids\"][0]) + max_new_tokens\n",
    "\n",
    "                input_tensors = torch.tensor(draft_input_ids[\"input_ids\"]).cuda(local_rank)\n",
    "            else:\n",
    "                output_len = len(new_inputs[0]) + max_new_tokens\n",
    "                input_tensors = new_inputs\n",
    "                if batch_size == 1:\n",
    "                    input_tensors.unsqueeze(0)\n",
    "\n",
    "            cat_tensor = draft_model.generate(input_tensors, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "            next_token_id = cat_tensor[:, -max_new_tokens:]\n",
    "\n",
    "            # Create a range tensor from 0 to max_new_tokens, which will be used to get a slice of length max_new_tokens+1\n",
    "            range_tensor = torch.arange(0, max_new_tokens).unsqueeze(0).expand(total_matched.size(0), -1).cuda(local_rank)\n",
    "\n",
    "            # Add the start positions to the range tensor to get the actual indices\n",
    "            indices = total_matched.unsqueeze(1) + range_tensor\n",
    "\n",
    "            # Now use torch.gather to get the slices from ground_truth_tensor\n",
    "            slices = torch.gather(ground_truth_tensor, 1, indices)\n",
    "\n",
    "            correct_predictions = (next_token_id == slices)\n",
    "\n",
    "            # Step 1: Convert the boolean tensor to float tensor\n",
    "            correct_predictions_float = correct_predictions.float()\n",
    "\n",
    "            # Step 2: Compute the cumulative sum\n",
    "            cumsum = correct_predictions_float.cumsum(dim=1)\n",
    "\n",
    "            # Step 3: Find the position of the first False (0) in each row\n",
    "            # If there is no False in the row, the position will be set to the length of the row\n",
    "            first_false_positions = torch.full((correct_predictions_float.size(0),), correct_predictions_float.size(1), device=correct_predictions_float.device)\n",
    "\n",
    "            # Find the positions of all False values.\n",
    "            false_positions = (correct_predictions_float == 0).nonzero(as_tuple=True)\n",
    "\n",
    "            # Update first_false_positions with the first False position for each row.\n",
    "            for row, col in zip(*false_positions):\n",
    "                first_false_positions[row] = min(first_false_positions[row], col)\n",
    "\n",
    "            # Compute the number of matched tokens in a batch\n",
    "            matched_tokens = first_false_positions + torch.ones_like(first_false_positions)\n",
    "\n",
    "            input_list = []\n",
    "\n",
    "            # Construct the input for the next iteration based on matched tokens in the current batch\n",
    "            for idx, matched in enumerate(matched_tokens):\n",
    "                input_list.append(torch.cat((torch.zeros(torch.max(matched_tokens) - matched_tokens[idx], dtype=torch.int32).cuda(local_rank), \n",
    "                                            input_tensors[idx], \n",
    "                                            ground_truth_tensor[idx][total_matched[idx]: total_matched[idx] + matched_tokens[idx]]), \n",
    "                                            dim=0))\n",
    "\n",
    "            new_inputs = torch.stack(input_list)\n",
    "            total_matched += matched_tokens\n",
    "\n",
    "            if local_rank == 0:\n",
    "                with open(output_file, \"a\") as f: # Replace with your file path\n",
    "                    f.write(str(total_matched) + str(\"\\n\"))\n",
    "\n",
    "            current_length = min(total_matched)\n",
    "\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac93a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
